{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training using Ablator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This chapter covers about training a model using Ablator with a popular **Fashion-mnist** dataset. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Experiments using Ablator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An experiment consists of a complete pipeline that involves loading configurations, training the model, and producing metrics. Ablator utilizes configurations, a model wrapper, and a trainer class to run an experiment. This is achieved by defining configurations and a model wrapper, and then passing them to the trainer class to start the experiment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up Ablator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install ablator using the command: ````pip install ablator````\n",
    "\n",
    "Import the **Configs**, **ModelWrapper** and **ProtoTrainer** from ablator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from ablator import ModelConfig, OptimizerConfig, TrainConfig, RunConfig\n",
    "from ablator import ModelWrapper, ProtoTrainer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from typing import Callable, Dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configurations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configurations has its own independent functions and serves specific purposes in the overall process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Defining Configs:\n",
    "\n",
    "- **Optimizer Config**: adam (lr = 0.001).\n",
    "- **Train Config**: batch_size = 32, epochs = 10, random weights initialization is set as true.\n",
    "- **Run Config**: device details, directory path and a random seed for experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_config = OptimizerConfig(\n",
    "    name=\"adam\", \n",
    "    arguments={\"lr\": 0.001}\n",
    ")\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    dataset=\"Fashion-mnist\",\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    optimizer_config=optimizer_config,\n",
    "    scheduler_config=None,\n",
    "    rand_weights_init = True\n",
    ")\n",
    "\n",
    "model_config = ModelConfig()\n",
    "\n",
    "# Random seed is used for generating same sequence of randomization every time.\n",
    "run_config = RunConfig(\n",
    "    train_config=train_config,\n",
    "    model_config=model_config,\n",
    "    metrics_n_batches = 800,\n",
    "    experiment_dir = \"/tmp/dir\",\n",
    "    device=\"cpu\",\n",
    "    amp=False,\n",
    "    random_seed = 42\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About the dataset\n",
    "\n",
    "**Fashion MNIST** is a dataset consisting of 60,000 grayscale images of fashion items. The images are categorized into ten classes, that includes clothing items. \n",
    "\n",
    "Image dimensions: 28 pixels x 28 pixels (grayscale)\n",
    "Shape of the training data tensor: [60000, 1, 28, 28]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating custom dataloaders.\n",
    "- In PyTorch, a DataLoader is a utility class that provides an iterable over a dataset. \n",
    "- It is commonly used for handling data loading and batching in machine learning and deep learning tasks. \n",
    "- Later, we will pass it to model wrapper. The wrapper will internally handle this for training and valuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Pytorch Model "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Architecture (Simple Neural Network with Linear Layers):\n",
    "\n",
    "Linear_1_(28*28, 256) -> ReLU -> Linear_2_(256, 256) -> ReLU -> Linear_3_(256, 10). (where, ReLU is an Activation function) \n",
    "\n",
    "````MyModel```` defines a model class that extends an existing model ````FashionMNISTModel````, adds a loss function, performs forward computation, and returns the predicted labels and loss during model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class FashionMNISTModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FashionMNISTModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "input_size = 28 * 28  \n",
    "hidden_size = 256\n",
    "num_classes = 10\n",
    "\n",
    "model = FashionMNISTModel(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Adding loss to the model.\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, config: ModelConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        out = self.model(x)\n",
    "        loss = None\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = self.loss(out, labels)\n",
    "\n",
    "        out = out.argmax(dim=-1)\n",
    "\n",
    "        return {\"y_pred\": out, \"y_true\": labels}, loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Custom Evaluation Metrics\n",
    "\n",
    "Defining evaluation functions for classification problems. Using average as \"weighted\" for multiclass evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_accuracy(y_true, y_pred):\n",
    "    return accuracy_score(y_true.flatten(), y_pred.flatten())\n",
    "\n",
    "def my_precision(y_true, y_pred):\n",
    "    return precision_score(y_true.flatten(), y_pred.flatten(), average='weighted')\n",
    "\n",
    "def my_recall(y_true, y_pred):\n",
    "    return recall_score(y_true.flatten(), y_pred.flatten(), average='weighted')\n",
    "\n",
    "def my_f1_score(y_true, y_pred):\n",
    "    return f1_score(y_true.flatten(), y_pred.flatten(), average='weighted')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Wrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This class serves as a comprehensive wrapper for PyTorch models, providing a high-level interface for handling various tasks involved in model training.\n",
    "\n",
    "- It takes care of importing parameters from configuration files into the model, setting up optimizers and schedulers and checkpoints, logging metrics, handling interruptions, creating and utilizing data loaders, evaluating model and much more.\n",
    "\n",
    "- By encapsulating these functionalities, it significantly reduces development efforts, minimizes the need for writing complex code, ultimately improving efficiency and productivity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating ModelWrapper and passing dataloaders and evaluation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyModelWrapper(ModelWrapper):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def make_dataloader_train(self, run_config: RunConfig):\n",
    "        return train_dataloader\n",
    "\n",
    "    def make_dataloader_val(self, run_config: RunConfig):\n",
    "        return test_dataloader\n",
    "\n",
    "    def evaluation_functions(self) -> Dict[str, Callable]:\n",
    "        return {\n",
    "            \"accuracy\": my_accuracy,\n",
    "            \"precision\": my_precision,\n",
    "            \"recall\": my_recall,\n",
    "            \"f1_score\": my_f1_score\n",
    "            }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ProtoTrainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This class is responsible to start training the model in the modelWrapper, preparing resources for model to avoid stalling during training or conficts between other trainers.\n",
    "\n",
    "- Provides logging and syncing facilities to the provided directory or external remote servers like google cloud etc. It also do evaluation and syncing metrics to the directories.\n",
    "\n",
    "- Therefore, to achieve this, it requires model wrapper and run config as inputs. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we wrap the model (**MyModel**) in a ModelWrapper (**MyModelWrapper**).\n",
    "Then, we create an instance of Prototrainer, passing the **run_config** and **wrapper** as arguments, and call the ````launch()```` to start the experiment.\n",
    "The ````launch()```` return metrics object of Class ````TrainMetrics````. It is used for calculates metrics for custom evaluation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "if not os.path.exists(run_config.experiment_dir):\n",
    "    shutil.os.mkdir(run_config.experiment_dir)\n",
    "\n",
    "shutil.rmtree(run_config.experiment_dir)\n",
    "\n",
    "wrapper = MyModelWrapper(\n",
    "    model_class=MyModel,\n",
    ")\n",
    "\n",
    "ablator = ProtoTrainer(\n",
    "    wrapper=wrapper,\n",
    "    run_config=run_config,\n",
    ")\n",
    "metrics = ablator.launch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TrainMetrics store and manages predictions and calculate metrics given evaluation functions. <br>\n",
    "We can get all the metrics from TrainMetrics using ````to_dict()```` method. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the metrics of custom evaluation functions which we had pass to the model Wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss        : 5.5833635228650795\n",
      "val_loss          : 13.444317814478225\n",
      "train_accuracy    : 0.827375\n",
      "train_f1_score    : 0.827292216086392\n",
      "train_precision   : 0.8272152550200749\n",
      "train_recall      : 0.827375\n",
      "val_accuracy      : 0.80342\n",
      "val_f1_score      : 0.8019798713403301\n",
      "val_precision     : 0.8130216190983056\n",
      "val_recall        : 0.80342\n",
      "best_iteration    : 16875\n",
      "best_loss         : 14.802360911396715\n",
      "current_epoch     : 10\n",
      "current_iteration : 18750\n",
      "epochs            : 10\n",
      "learning_rate     : 0.001\n",
      "total_steps       : 18750\n"
     ]
    }
   ],
   "source": [
    "metrics_dict = metrics.to_dict()\n",
    "max_key_length = max(len(str(k)) for k in metrics_dict.keys())\n",
    "\n",
    "for k, v in metrics_dict.items():\n",
    "    print(f\"{k:{max_key_length}} : {v}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why training with ProtoTrainer?\n",
    "\n",
    "- It provides a robust way to handle errors during training.\n",
    "- Ideal for prototyping experiments in a local environment.\n",
    "- Easily adaptable for hyperparameter optimization with larger configurations and horizontal scaling.\n",
    "- Quick transition to \"ParallelConfig\" and \"ParallelTrainer\" for parallel execution of trials using Ray."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to visualize metrics\n",
    "\n",
    "- We can also visualize metrics on TensorBoard with respect to every epoch.\n",
    "- Just install ````tensorboard````. Load using ````%load_ext tensorboard```` if using notebook.\n",
    "- Run the command %tensorboard --logdir /tmp/dir/<Experiment_dir_name>/dashboard/tensorboard --port [port]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
