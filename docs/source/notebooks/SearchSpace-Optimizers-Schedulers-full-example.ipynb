{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieMagiCsNw6O"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import ablator\n",
        "except:\n",
        "    !pip install git+https://github.com/fostiropoulos/ablator.git@v0.0.1-mp\n",
        "    print(\"Stopping RUNTIME! Please run again\") # This script automatically restart runtime (if ablator is not found and installing is needed) so changes are applied\n",
        "    import os\n",
        "\n",
        "    os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvQRz4GzQBpG"
      },
      "source": [
        "# Import neccesary modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4JGSrAgbOsK1"
      },
      "outputs": [],
      "source": [
        "from ablator import ModelConfig, TrainConfig, ParallelConfig, SchedulerConfig\n",
        "from ablator import ModelWrapper, ParallelTrainer, configclass, ConfigBase, Literal, Optional\n",
        "from ablator.config.hpo import SearchSpace\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim.lr_scheduler import OneCycleLR, ReduceLROnPlateau, StepLR\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMzFcM6RPmy8"
      },
      "source": [
        "# 1 - Configurations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BNVl37zPQS0"
      },
      "source": [
        "## 1.1 - Model configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hzMQjne5Nly8"
      },
      "outputs": [],
      "source": [
        "@configclass\n",
        "class CustomModelConfig(ModelConfig):\n",
        "  num_filter1: int\n",
        "  num_filter2: int\n",
        "  activation: str\n",
        "\n",
        "model_config = CustomModelConfig(\n",
        "    num_filter1 =32,\n",
        "    num_filter2 = 64,\n",
        "    activation = \"relu\"\n",
        ")\n",
        "\n",
        "class FashionCNN(nn.Module):\n",
        "    def __init__(self, config: CustomModelConfig):\n",
        "        super(FashionCNN, self).__init__()\n",
        "\n",
        "        activation_list = {\"relu\": nn.ReLU(), \"elu\": nn.ELU(), \"leakyRelu\": nn.LeakyReLU()}\n",
        "\n",
        "        num_filter1 = config.num_filter1\n",
        "        num_filter2 = config.num_filter2\n",
        "        activation = activation_list[config.activation]\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, num_filter1, kernel_size=3, stride=1, padding=1)\n",
        "        self.act1 = activation\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(num_filter1, num_filter2, kernel_size=3, stride=1, padding=1)\n",
        "        self.act2 = activation\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(num_filter2, num_filter2, kernel_size=3, stride=1, padding=1)\n",
        "        self.act3 = activation\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(num_filter2 * 7 * 7, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.act2(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.act3(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, config: CustomModelConfig) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = FashionCNN(config)\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x, labels=None):\n",
        "        out = self.model(x)\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss = self.loss(out, labels)\n",
        "            labels = labels.reshape(-1, 1)\n",
        "\n",
        "        out = out.argmax(dim=-1)\n",
        "        out = out.reshape(-1, 1)\n",
        "\n",
        "        return {\"y_pred\": out, \"y_true\": labels}, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c52lsrH6PSgm"
      },
      "source": [
        "## 1.2 - Train configs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPbpg-DEPUBy"
      },
      "source": [
        "### 1.2.1 - Optimizer and scheduler configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NID2QgxYPOjD"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CustomOptimizerConfig(name='adam', lr=0.001)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def create_optimizer(optimizer_name: str, model: nn.Module, lr: float):\n",
        "\n",
        "    parameter_groups = [v for k, v in model.named_parameters()]\n",
        "\n",
        "    adamw_parameters = {\n",
        "      \"betas\": (0.0, 0.1),\n",
        "      \"eps\": 0.001,\n",
        "      \"weight_decay\": 0.1\n",
        "    }\n",
        "    adam_parameters = {\n",
        "      \"betas\" : (0.0, 0.1),\n",
        "      \"weight_decay\": 0.0\n",
        "    }\n",
        "    sgd_parameters = {\n",
        "      \"momentum\": 0.9,\n",
        "      \"weight_decay\": 0.1\n",
        "    }\n",
        "\n",
        "    Optimizer = None\n",
        "\n",
        "    if optimizer_name == \"adam\":\n",
        "        Optimizer = optim.Adam(parameter_groups, lr = lr, **adam_parameters)\n",
        "    elif optimizer_name == \"adamw\":\n",
        "        Optimizer = optim.AdamW(parameter_groups, lr = lr, **adamw_parameters)\n",
        "    elif optimizer_name == \"sgd\":\n",
        "        Optimizer = optim.SGD(parameter_groups, lr = lr, **sgd_parameters)\n",
        "\n",
        "    return Optimizer\n",
        "\n",
        "@configclass\n",
        "class CustomOptimizerConfig(ConfigBase):\n",
        "    name: Literal[\"adam\", \"adamw\", \"sgd\"] = \"adam\"\n",
        "    lr: float = 0.001\n",
        "\n",
        "    def make_optimizer(self, model: nn.Module):\n",
        "        return create_optimizer(self.name, model, self.lr)\n",
        "\n",
        "optimizer_config = CustomOptimizerConfig(name = \"adam\", lr = 0.001)\n",
        "optimizer_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CustomSchedulerConfig(name='step', arguments={'step_size': 1, 'gamma': 0.99, 'step_when': 'epoch'})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def create_scheduler(scheduler_name: str, model: nn.Module, optimizer: torch.optim):\n",
        "\n",
        "  parameters = scheduler_arguments(scheduler_name)\n",
        "  del parameters[\"step_when\"]\n",
        "\n",
        "  Scheduler = None\n",
        "\n",
        "  if scheduler_name == \"step\":\n",
        "    Scheduler = StepLR(optimizer, **parameters)\n",
        "  elif scheduler_name == \"cycle\":\n",
        "    Scheduler = OneCycleLR(optimizer, **parameters)\n",
        "  elif scheduler_name == \"plateau\":\n",
        "    Scheduler = ReduceLROnPlateau(optimizer, **parameters)\n",
        "\n",
        "  return Scheduler\n",
        "\n",
        "def scheduler_arguments(scheduler_name):\n",
        "  if scheduler_name == \"step\":\n",
        "    return {\n",
        "      \"step_size\" : 1,\n",
        "      \"gamma\" : 0.99,\n",
        "      \"step_when\": \"epoch\"\n",
        "    }\n",
        "  elif scheduler_name == \"plateau\":\n",
        "    return {\n",
        "      \"patience\":  10,\n",
        "      \"min_lr\":  1e-5,\n",
        "      \"mode\":  \"min\",\n",
        "      \"factor\":   0.0,\n",
        "      \"threshold\":  1e-4,\n",
        "      \"step_when\": \"val\"\n",
        "    }\n",
        "  elif scheduler_name == \"cycle\":\n",
        "    return {\n",
        "      \"max_lr\": 1e-3,\n",
        "      \"total_steps\": 7 * 1875,  # n.o epochs * len(dataloader)\n",
        "      \"step_when\": \"train\"\n",
        "    }\n",
        "\n",
        "@configclass\n",
        "class CustomSchedulerConfig(SchedulerConfig):\n",
        "    def __init__(self, name, arguments=None):\n",
        "        arguments = scheduler_arguments(name)\n",
        "        super(CustomSchedulerConfig, self).__init__(name=name, arguments=arguments)\n",
        "\n",
        "    def make_scheduler(self, model: torch.nn.Module, optimizer: torch.optim):\n",
        "        return create_scheduler(self.name, model, optimizer)\n",
        "\n",
        "scheduler_config = CustomSchedulerConfig(name = \"step\")\n",
        "scheduler_config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Pi2w-GAPW2H"
      },
      "source": [
        "### 1.2.2 - Train config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MGR8dyLbO1ko"
      },
      "outputs": [],
      "source": [
        "@configclass\n",
        "class CustomTrainConfig(TrainConfig):\n",
        "  optimizer_config: CustomOptimizerConfig\n",
        "  scheduler_config: Optional[CustomSchedulerConfig]\n",
        "\n",
        "train_config = CustomTrainConfig(\n",
        "    dataset=\"Fashion-mnist\",\n",
        "    batch_size=32,\n",
        "    epochs=7,\n",
        "    optimizer_config=optimizer_config,\n",
        "    scheduler_config=scheduler_config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxD3tri7PcH2"
      },
      "source": [
        "## 1.3 - Run config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ISPJS-YSPdxs"
      },
      "outputs": [],
      "source": [
        "search_space = {\n",
        "    \"model_config.num_filter1\": SearchSpace(value_range = [32, 64], value_type = 'int'),\n",
        "    \"model_config.num_filter2\": SearchSpace(value_range = [64, 128], value_type = 'int'),\n",
        "    \"train_config.optimizer_config.lr\": SearchSpace(value_range = [0.001, 0.01], value_type = 'float'),\n",
        "    \"train_config.optimizer_config.name\": SearchSpace(categorical_values = [\"adam\", \"sgd\", \"adamw\"]),\n",
        "    \"train_config.scheduler_config.name\": SearchSpace(categorical_values = [\"cycle\", \"step\", \"plateau\"]),\n",
        "    \"model_config.activation\": SearchSpace(categorical_values = [\"relu\", \"elu\", \"leakyRelu\"]),\n",
        "}\n",
        "\n",
        "@configclass\n",
        "class CustomParallelConfig(ParallelConfig):\n",
        "  model_config: CustomModelConfig\n",
        "  train_config: CustomTrainConfig\n",
        "\n",
        "parallel_config = CustomParallelConfig(\n",
        "    train_config=train_config,\n",
        "    model_config=model_config,\n",
        "    metrics_n_batches = 300,\n",
        "    experiment_dir = \"/tmp/experiments-1/\",\n",
        "    device=\"cuda\",\n",
        "    amp=True,\n",
        "    random_seed = 42,\n",
        "    total_trials = 3,   # increase this based on your available resources\n",
        "    concurrent_trials = 1,  # increase this based on your available resources\n",
        "    search_space = search_space,\n",
        "    optim_metrics = {\"val_loss\": \"min\"},\n",
        "    optim_metric_name = \"val_loss\",\n",
        "    gpu_mb_per_experiment = 512,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWmteXS7Phdc"
      },
      "source": [
        "# 2 - Model wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppiqJ6_IPuT4",
        "outputId": "6c200ae8-7e16-45e0-9a93-d476d13f9faa"
      },
      "outputs": [],
      "source": [
        "transform = transforms.ToTensor()\n",
        "\n",
        "train_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "class MyModelWrapper(ModelWrapper):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def make_dataloader_train(self, run_config: CustomParallelConfig):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=run_config.train_config.batch_size,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "    def make_dataloader_val(self, run_config: CustomParallelConfig):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=run_config.train_config.batch_size,\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "    def evaluation_functions(self):\n",
        "        return {\n",
        "            \"accuracy\": lambda y_true, y_pred: accuracy_score(y_true.flatten(), y_pred.flatten()),\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz3TV9rbPw_1"
      },
      "source": [
        "# 3 - Launch experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf8sGaKJPyvq",
        "outputId": "249fc549-9ee5-488d-c809-d749bb586ba9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-22 22:16:19,693\tWARNING packaging.py:394 -- File /content/data/FashionMNIST/raw/train-images-idx3-ubyte.gz is very large (25.20MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/content/data/FashionMNIST/raw/train-images-idx3-ubyte.gz']})`\n",
            "2023-09-22 22:16:19,812\tWARNING packaging.py:394 -- File /content/data/FashionMNIST/raw/train-images-idx3-ubyte is very large (44.86MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/content/data/FashionMNIST/raw/train-images-idx3-ubyte']})`\n",
            "2023-09-22 22:16:20,050\tWARNING packaging.py:394 -- File /content/sample_data/mnist_train_small.csv is very large (34.83MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/content/sample_data/mnist_train_small.csv']})`\n",
            "2023-09-22 22:16:20,191\tWARNING packaging.py:394 -- File /content/sample_data/mnist_test.csv is very large (17.44MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/content/sample_data/mnist_test.csv']})`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-09-22 22:16:49:  - \u001b[93mNo git repository was detected at /content. We recommend setting the working directory to a git repository to keep track of changes.\u001b[0m\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m 2023-09-22 22:16:49:  - \u001b[93mNo git repository was detected at /content. We recommend setting the working directory to a git repository to keep track of changes.\u001b[0m\n",
            "2023-09-22 22:16:49:  - Scheduling uid: bc16_d3c6_9019\n",
            "Parameters: \n",
            "\ttrain_config.optimizer_config.lr:(float)0.001->(float)0.007367092824298474\n",
            "\ttrain_config.scheduler_config.arguments.patience:(Missing)None->(int)10\n",
            "\ttrain_config.scheduler_config.arguments.min_lr:(Missing)None->(float)1e-05\n",
            "\ttrain_config.scheduler_config.arguments.step_size:(int)1->(Missing)None\n",
            "\texperiment_dir:(str)/content/experiments/->(str)/content/experiments/bc16_d3c6_9019\n",
            "\ttrain_config.scheduler_config.arguments.threshold:(Missing)None->(float)0.0001\n",
            "\ttrain_config.scheduler_config.arguments.verbose:(Missing)None->(bool)False\n",
            "\tmodel_config.num_filter1:(int)32->(int)54\n",
            "\ttrain_config.scheduler_config.arguments.step_when:(str)epoch->(str)val\n",
            "\ttrain_config.scheduler_config.name:(str)step->(str)plateau\n",
            "\ttrain_config.scheduler_config.arguments.mode:(Missing)None->(str)min\n",
            "\tmodel_config.num_filter2:(int)64->(int)108\n",
            "\ttrain_config.scheduler_config.arguments.gamma:(float)0.99->(Missing)None\n",
            "\ttrain_config.scheduler_config.arguments.factor:(Missing)None->(float)0.0\n",
            "-----\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m 2023-09-22 22:16:49:  - Scheduling uid: bc16_d3c6_9019\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m Parameters: \n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \ttrain_config.optimizer_config.lr:(float)0.001->(float)0.007367092824298474\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \ttrain_config.scheduler_config.arguments.patience:(Missing)None->(int)10\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \ttrain_config.scheduler_config.arguments.min_lr:(Missing)None->(float)1e-05\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \ttrain_config.scheduler_config.arguments.step_size:(int)1->(Missing)None\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \texperiment_dir:(str)/content/experiments/->(str)/content/experiments/bc16_d3c6_9019\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \ttrain_config.scheduler_config.arguments.threshold:(Missing)None->(float)0.0001\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \ttrain_config.scheduler_config.arguments.verbose:(Missing)None->(bool)False\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \tmodel_config.num_filter1:(int)32->(int)54\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \ttrain_config.scheduler_config.arguments.step_when:(str)epoch->(str)val\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \ttrain_config.scheduler_config.name:(str)step->(str)plateau\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \ttrain_config.scheduler_config.arguments.mode:(Missing)None->(str)min\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \tmodel_config.num_filter2:(int)64->(int)108\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \ttrain_config.scheduler_config.arguments.gamma:(float)0.99->(Missing)None\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \ttrain_config.scheduler_config.arguments.factor:(Missing)None->(float)0.0\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m -----\n",
            "\u001b[2m\u001b[36m(bc16_d3c6_9019 pid=1230)\u001b[0m 2023-09-22 22:16:57: Model directory: /content/experiments/bc16_d3c6_9019\n",
            "\u001b[2m\u001b[36m(bc16_d3c6_9019 pid=1230)\u001b[0m 2023-09-22 22:16:57: Creating new model\n",
            "2023-09-22 22:17:02:  - Scheduling uid: 7101_4da6_8348\n",
            "Parameters: \n",
            "\ttrain_config.optimizer_config.lr:(float)0.001->(float)0.0076130418635819205\n",
            "\texperiment_dir:(str)/content/experiments/->(str)/content/experiments/7101_4da6_8348\n",
            "\tmodel_config.num_filter1:(int)32->(int)56\n",
            "\ttrain_config.optimizer_config.name:(str)adam->(str)adamw\n",
            "\tmodel_config.num_filter2:(int)64->(int)73\n",
            "-----\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m 2023-09-22 22:17:02:  - Scheduling uid: 7101_4da6_8348\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m Parameters: \n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \ttrain_config.optimizer_config.lr:(float)0.001->(float)0.0076130418635819205\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \texperiment_dir:(str)/content/experiments/->(str)/content/experiments/7101_4da6_8348\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \tmodel_config.num_filter1:(int)32->(int)56\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \ttrain_config.optimizer_config.name:(str)adam->(str)adamw\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \tmodel_config.num_filter2:(int)64->(int)73\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m -----\n",
            "\u001b[2m\u001b[36m(7101_4da6_8348 pid=1354)\u001b[0m 2023-09-22 22:17:16: Model directory: /content/experiments/7101_4da6_8348\n",
            "\u001b[2m\u001b[36m(7101_4da6_8348 pid=1354)\u001b[0m 2023-09-22 22:17:16: Creating new model\n",
            "\u001b[2m\u001b[36m(bc16_d3c6_9019 pid=1230)\u001b[0m 2023-09-22 22:18:01: Evaluation Step [1] val_loss: 2.70e-01 val_accuracy: 0.822000 train_loss: 6.44e-01 best_iteration: 00001875 best_val_loss: 2.70e-01 current_epoch: 00000001 current_iteration: 00001875 epochs: 00000007 learning_rate: 7.37e-03 total_steps: 00013125 train_accuracy: nan\n",
            "\u001b[2m\u001b[36m(bc16_d3c6_9019 pid=1230)\u001b[0m 2023-09-22 22:18:01: val_loss: 2.70e-01 val_accuracy: 0.822000 train_loss: 6.44e-01 best_iteration: 00001875 best_val_loss: 2.70e-01 current_epoch: 00000001 current_iteration: 00001875 epochs: 00000007 learning_rate: 7.37e-03 total_steps: 00013125 train_accuracy: 0.810000\n",
            "\u001b[2m\u001b[36m(7101_4da6_8348 pid=1354)\u001b[0m 2023-09-22 22:18:07: val_loss: 2.02e-01 val_accuracy: 0.854500 train_loss: 4.73e-01 best_iteration: 00001875 best_val_loss: 2.02e-01 current_epoch: 00000001 current_iteration: 00001875 epochs: 00000007 learning_rate: 7.61e-03 total_steps: 00013125 train_accuracy: 8.53e-01\n",
            "\u001b[2m\u001b[36m(7101_4da6_8348 pid=1354)\u001b[0m 2023-09-22 22:18:07: val_loss: 2.02e-01 val_accuracy: 0.854500 train_loss: 4.73e-01 best_iteration: 00001875 best_val_loss: 2.02e-01 current_epoch: 00000001 current_iteration: 00001875 epochs: 00000007 learning_rate: 7.61e-03 total_steps: 00013125 train_accuracy: 8.53e-01\n",
            "\u001b[2m\u001b[36m(bc16_d3c6_9019 pid=1230)\u001b[0m 2023-09-22 22:18:51: Evaluation Step [2] val_loss: 4.79e-01 val_accuracy: 0.821700 train_loss: 6.34e-01 best_iteration: 00001875 best_val_loss: 2.70e-01 current_epoch: 00000002 current_iteration: 00003750 epochs: 00000007 learning_rate: 7.37e-03 total_steps: 00013125 train_accuracy: 0.810000\n",
            "\u001b[2m\u001b[36m(bc16_d3c6_9019 pid=1230)\u001b[0m 2023-09-22 22:18:51: val_loss: 4.79e-01 val_accuracy: 0.821700 train_loss: 6.34e-01 best_iteration: 00001875 best_val_loss: 2.70e-01 current_epoch: 00000002 current_iteration: 00003750 epochs: 00000007 learning_rate: 7.37e-03 total_steps: 00013125 train_accuracy: 0.860000\n",
            "\u001b[2m\u001b[36m(7101_4da6_8348 pid=1354)\u001b[0m 2023-09-22 22:18:57: val_loss: 2.68e-01 val_accuracy: 0.867300 train_loss: 3.76e-01 best_iteration: 00001875 best_val_loss: 2.02e-01 current_epoch: 00000002 current_iteration: 00003750 epochs: 00000007 learning_rate: 7.46e-03 total_steps: 00013125 train_accuracy: 8.53e-01\n",
            "\u001b[2m\u001b[36m(7101_4da6_8348 pid=1354)\u001b[0m 2023-09-22 22:18:57: val_loss: 2.68e-01 val_accuracy: 0.867300 train_loss: 3.76e-01 best_iteration: 00001875 best_val_loss: 2.02e-01 current_epoch: 00000002 current_iteration: 00003750 epochs: 00000007 learning_rate: 7.46e-03 total_steps: 00013125 train_accuracy: 8.53e-01\n",
            "\u001b[2m\u001b[36m(bc16_d3c6_9019 pid=1230)\u001b[0m 2023-09-22 22:19:37: Evaluation Step [3] val_loss: 4.65e-01 val_accuracy: 0.798600 train_loss: 6.25e-01 best_iteration: 00001875 best_val_loss: 2.70e-01 current_epoch: 00000003 current_iteration: 00005625 epochs: 00000007 learning_rate: 7.37e-03 total_steps: 00013125 train_accuracy: 0.860000\n",
            "\u001b[2m\u001b[36m(bc16_d3c6_9019 pid=1230)\u001b[0m 2023-09-22 22:19:37: val_loss: 4.65e-01 val_accuracy: 0.798600 train_loss: 6.25e-01 best_iteration: 00001875 best_val_loss: 2.70e-01 current_epoch: 00000003 current_iteration: 00005625 epochs: 00000007 learning_rate: 7.37e-03 total_steps: 00013125 train_accuracy: 0.790000\n",
            "\u001b[2m\u001b[36m(7101_4da6_8348 pid=1354)\u001b[0m 2023-09-22 22:19:44: val_loss: 1.61e-01 val_accuracy: 0.870400 train_loss: 3.57e-01 best_iteration: 00005625 best_val_loss: 1.61e-01 current_epoch: 00000003 current_iteration: 00005625 epochs: 00000007 learning_rate: 7.31e-03 total_steps: 00013125 train_accuracy: 0.880000\n",
            "\u001b[2m\u001b[36m(7101_4da6_8348 pid=1354)\u001b[0m 2023-09-22 22:19:44: val_loss: 1.61e-01 val_accuracy: 0.870400 train_loss: 3.57e-01 best_iteration: 00005625 best_val_loss: 1.61e-01 current_epoch: 00000003 current_iteration: 00005625 epochs: 00000007 learning_rate: 7.31e-03 total_steps: 00013125 train_accuracy: 0.880000\n",
            "\u001b[2m\u001b[36m(bc16_d3c6_9019 pid=1230)\u001b[0m 2023-09-22 22:20:30: Evaluation Step [4] val_loss: 4.09e-01 val_accuracy: 0.811100 train_loss: 6.26e-01 best_iteration: 00001875 best_val_loss: 2.70e-01 current_epoch: 00000004 current_iteration: 00007500 epochs: 00000007 learning_rate: 7.37e-03 total_steps: 00013125 train_accuracy: 0.790000\n",
            "\u001b[2m\u001b[36m(bc16_d3c6_9019 pid=1230)\u001b[0m 2023-09-22 22:20:30: val_loss: 4.09e-01 val_accuracy: 0.811100 train_loss: 6.26e-01 best_iteration: 00001875 best_val_loss: 2.70e-01 current_epoch: 00000004 current_iteration: 00007500 epochs: 00000007 learning_rate: 7.37e-03 total_steps: 00013125 train_accuracy: 8.43e-01\n",
            "\u001b[2m\u001b[36m(7101_4da6_8348 pid=1354)\u001b[0m 2023-09-22 22:20:35: val_loss: 2.48e-01 val_accuracy: 0.842500 train_loss: 3.53e-01 best_iteration: 00005625 best_val_loss: 1.61e-01 current_epoch: 00000004 current_iteration: 00007500 epochs: 00000007 learning_rate: 7.17e-03 total_steps: 00013125 train_accuracy: 8.87e-01\n",
            "\u001b[2m\u001b[36m(7101_4da6_8348 pid=1354)\u001b[0m 2023-09-22 22:20:35: val_loss: 2.48e-01 val_accuracy: 0.842500 train_loss: 3.53e-01 best_iteration: 00005625 best_val_loss: 1.61e-01 current_epoch: 00000004 current_iteration: 00007500 epochs: 00000007 learning_rate: 7.17e-03 total_steps: 00013125 train_accuracy: 8.87e-01\n",
            "\u001b[2m\u001b[36m(bc16_d3c6_9019 pid=1230)\u001b[0m 2023-09-22 22:21:15: Evaluation Step [5] val_loss: 5.42e-01 val_accuracy: 0.820500 train_loss: 6.63e-01 best_iteration: 00001875 best_val_loss: 2.70e-01 current_epoch: 00000005 current_iteration: 00009375 epochs: 00000007 learning_rate: 7.37e-03 total_steps: 00013125 train_accuracy: 8.43e-01\n",
            "\u001b[2m\u001b[36m(bc16_d3c6_9019 pid=1230)\u001b[0m 2023-09-22 22:21:15: val_loss: 5.42e-01 val_accuracy: 0.820500 train_loss: 6.63e-01 best_iteration: 00001875 best_val_loss: 2.70e-01 current_epoch: 00000005 current_iteration: 00009375 epochs: 00000007 learning_rate: 7.37e-03 total_steps: 00013125 train_accuracy: 8.17e-01\n",
            "\u001b[2m\u001b[36m(7101_4da6_8348 pid=1354)\u001b[0m 2023-09-22 22:21:25: val_loss: 2.15e-01 val_accuracy: 0.879700 train_loss: 3.47e-01 best_iteration: 00005625 best_val_loss: 1.61e-01 current_epoch: 00000005 current_iteration: 00009375 epochs: 00000007 learning_rate: 7.02e-03 total_steps: 00013125 train_accuracy: 0.890000\n",
            "\u001b[2m\u001b[36m(7101_4da6_8348 pid=1354)\u001b[0m 2023-09-22 22:21:25: val_loss: 2.15e-01 val_accuracy: 0.879700 train_loss: 3.47e-01 best_iteration: 00005625 best_val_loss: 1.61e-01 current_epoch: 00000005 current_iteration: 00009375 epochs: 00000007 learning_rate: 7.02e-03 total_steps: 00013125 train_accuracy: 0.890000\n",
            "\u001b[2m\u001b[36m(bc16_d3c6_9019 pid=1230)\u001b[0m 2023-09-22 22:22:02: Evaluation Step [6] val_loss: 3.81e-01 val_accuracy: 0.796600 train_loss: 6.93e-01 best_iteration: 00001875 best_val_loss: 2.70e-01 current_epoch: 00000006 current_iteration: 00011250 epochs: 00000007 learning_rate: 7.37e-03 total_steps: 00013125 train_accuracy: 8.17e-01\n",
            "\u001b[2m\u001b[36m(bc16_d3c6_9019 pid=1230)\u001b[0m 2023-09-22 22:22:02: val_loss: 3.81e-01 val_accuracy: 0.796600 train_loss: 6.93e-01 best_iteration: 00001875 best_val_loss: 2.70e-01 current_epoch: 00000006 current_iteration: 00011250 epochs: 00000007 learning_rate: 7.37e-03 total_steps: 00013125 train_accuracy: 8.13e-01\n",
            "\u001b[2m\u001b[36m(7101_4da6_8348 pid=1354)\u001b[0m 2023-09-22 22:22:07: val_loss: 2.59e-01 val_accuracy: 0.880500 train_loss: 3.41e-01 best_iteration: 00005625 best_val_loss: 1.61e-01 current_epoch: 00000006 current_iteration: 00011250 epochs: 00000007 learning_rate: 6.89e-03 total_steps: 00013125 train_accuracy: 8.93e-01\n",
            "\u001b[2m\u001b[36m(7101_4da6_8348 pid=1354)\u001b[0m 2023-09-22 22:22:07: val_loss: 2.59e-01 val_accuracy: 0.880500 train_loss: 3.41e-01 best_iteration: 00005625 best_val_loss: 1.61e-01 current_epoch: 00000006 current_iteration: 00011250 epochs: 00000007 learning_rate: 6.89e-03 total_steps: 00013125 train_accuracy: 8.93e-01\n",
            "\u001b[2m\u001b[36m(bc16_d3c6_9019 pid=1230)\u001b[0m 2023-09-22 22:22:55: Evaluation Step [7] val_loss: 4.20e-01 val_accuracy: 0.809300 train_loss: 6.73e-01 best_iteration: 00001875 best_val_loss: 2.70e-01 current_epoch: 00000007 current_iteration: 00013125 epochs: 00000007 learning_rate: 7.37e-03 total_steps: 00013125 train_accuracy: 8.13e-01\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m 2023-09-22 22:22:55:  - Finished training - bc16_d3c6\n",
            "\u001b[2m\u001b[36m(bc16_d3c6_9019 pid=1230)\u001b[0m 2023-09-22 22:22:55: val_loss: 4.20e-01 val_accuracy: 0.809300 train_loss: 6.73e-01 best_iteration: 00001875 best_val_loss: 2.70e-01 current_epoch: 00000007 current_iteration: 00013125 epochs: 00000007 learning_rate: 7.37e-03 total_steps: 00013125 train_accuracy: 7.87e-01\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m 2023-09-22 22:23:02:  - Finished training - 7101_4da6\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
            "\u001b[2m\u001b[36m(7101_4da6_8348 pid=1354)\u001b[0m 2023-09-22 22:23:02: Evaluation Step [7] val_loss: 2.61e-01 val_accuracy: 0.874100 train_loss: 3.38e-01 best_iteration: 00005625 best_val_loss: 1.61e-01 current_epoch: 00000007 current_iteration: 00013125 epochs: 00000007 learning_rate: 6.75e-03 total_steps: 00013125 train_accuracy: 8.93e-01\n",
            "\u001b[2m\u001b[36m(7101_4da6_8348 pid=1354)\u001b[0m 2023-09-22 22:23:02: val_loss: 2.61e-01 val_accuracy: 0.874100 train_loss: 3.38e-01 best_iteration: 00005625 best_val_loss: 1.61e-01 current_epoch: 00000007 current_iteration: 00013125 epochs: 00000007 learning_rate: 6.75e-03 total_steps: 00013125 train_accuracy: 8.57e-01\n",
            "2023-09-22 22:23:05:  - Scheduling uid: 179d_789f_134e\n",
            "Parameters: \n",
            "\ttrain_config.optimizer_config.lr:(float)0.001->(float)0.008541406890769248\n",
            "\ttrain_config.scheduler_config.arguments.max_lr:(Missing)None->(float)0.001\n",
            "\ttrain_config.scheduler_config.arguments.step_size:(int)1->(Missing)None\n",
            "\texperiment_dir:(str)/content/experiments/->(str)/content/experiments/179d_789f_134e\n",
            "\tmodel_config.num_filter1:(int)32->(int)33\n",
            "\ttrain_config.scheduler_config.arguments.step_when:(str)epoch->(str)train\n",
            "\ttrain_config.optimizer_config.name:(str)adam->(str)sgd\n",
            "\ttrain_config.scheduler_config.name:(str)step->(str)cycle\n",
            "\tmodel_config.num_filter2:(int)64->(int)95\n",
            "\ttrain_config.scheduler_config.arguments.gamma:(float)0.99->(Missing)None\n",
            "\tmodel_config.activation:(str)relu->(str)elu\n",
            "\ttrain_config.scheduler_config.arguments.total_steps:(Missing)None->(int)13132\n",
            "-----\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m 2023-09-22 22:23:05:  - Scheduling uid: 179d_789f_134e\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m Parameters: \n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \ttrain_config.optimizer_config.lr:(float)0.001->(float)0.008541406890769248\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \ttrain_config.scheduler_config.arguments.max_lr:(Missing)None->(float)0.001\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \ttrain_config.scheduler_config.arguments.step_size:(int)1->(Missing)None\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \texperiment_dir:(str)/content/experiments/->(str)/content/experiments/179d_789f_134e\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \tmodel_config.num_filter1:(int)32->(int)33\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \ttrain_config.scheduler_config.arguments.step_when:(str)epoch->(str)train\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \ttrain_config.optimizer_config.name:(str)adam->(str)sgd\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \ttrain_config.scheduler_config.name:(str)step->(str)cycle\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \tmodel_config.num_filter2:(int)64->(int)95\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \ttrain_config.scheduler_config.arguments.gamma:(float)0.99->(Missing)None\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \tmodel_config.activation:(str)relu->(str)elu\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m \ttrain_config.scheduler_config.arguments.total_steps:(Missing)None->(int)13132\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m -----\n",
            "\u001b[2m\u001b[36m(179d_789f_134e pid=3591)\u001b[0m 2023-09-22 22:23:11: Model directory: /content/experiments/179d_789f_134e\n",
            "\u001b[2m\u001b[36m(179d_789f_134e pid=3591)\u001b[0m 2023-09-22 22:23:11: Creating new model\n",
            "\u001b[2m\u001b[36m(7101_4da6_8348 pid=1354)\u001b[0m 2023-09-22 22:23:11: Creating new model\n",
            "\u001b[2m\u001b[36m(179d_789f_134e pid=3591)\u001b[0m 2023-09-22 22:23:44: Evaluation Step [1] val_loss: 6.11e-01 val_accuracy: 0.708700 train_loss: 1.45e+00 best_iteration: 00001875 best_val_loss: 6.11e-01 current_epoch: 00000001 current_iteration: 00001875 epochs: 00000007 learning_rate: 4.84e-04 total_steps: 00013125 train_accuracy: nan\n",
            "\u001b[2m\u001b[36m(179d_789f_134e pid=3591)\u001b[0m 2023-09-22 22:23:44: val_loss: 6.11e-01 val_accuracy: 0.708700 train_loss: 1.45e+00 best_iteration: 00001875 best_val_loss: 6.11e-01 current_epoch: 00000001 current_iteration: 00001875 epochs: 00000007 learning_rate: 4.84e-04 total_steps: 00013125 train_accuracy: 6.87e-01\n",
            "\u001b[2m\u001b[36m(179d_789f_134e pid=3591)\u001b[0m 2023-09-22 22:24:10: Evaluation Step [2] val_loss: 5.94e-01 val_accuracy: 0.720900 train_loss: 8.43e-01 best_iteration: 00003750 best_val_loss: 5.94e-01 current_epoch: 00000002 current_iteration: 00003750 epochs: 00000007 learning_rate: 9.95e-04 total_steps: 00013125 train_accuracy: 6.87e-01\n",
            "\u001b[2m\u001b[36m(179d_789f_134e pid=3591)\u001b[0m 2023-09-22 22:24:10: val_loss: 5.94e-01 val_accuracy: 0.720900 train_loss: 8.43e-01 best_iteration: 00003750 best_val_loss: 5.94e-01 current_epoch: 00000002 current_iteration: 00003750 epochs: 00000007 learning_rate: 9.95e-04 total_steps: 00013125 train_accuracy: 6.87e-01\n",
            "\u001b[2m\u001b[36m(179d_789f_134e pid=3591)\u001b[0m 2023-09-22 22:24:35: Evaluation Step [3] val_loss: 6.37e-01 val_accuracy: 0.703600 train_loss: 8.48e-01 best_iteration: 00003750 best_val_loss: 5.94e-01 current_epoch: 00000003 current_iteration: 00005625 epochs: 00000007 learning_rate: 9.19e-04 total_steps: 00013125 train_accuracy: 6.87e-01\n",
            "\u001b[2m\u001b[36m(179d_789f_134e pid=3591)\u001b[0m 2023-09-22 22:24:35: val_loss: 6.37e-01 val_accuracy: 0.703600 train_loss: 8.48e-01 best_iteration: 00003750 best_val_loss: 5.94e-01 current_epoch: 00000003 current_iteration: 00005625 epochs: 00000007 learning_rate: 9.19e-04 total_steps: 00013125 train_accuracy: 7.27e-01\n",
            "\u001b[2m\u001b[36m(179d_789f_134e pid=3591)\u001b[0m 2023-09-22 22:25:00: Evaluation Step [4] val_loss: 6.11e-01 val_accuracy: 0.701700 train_loss: 8.60e-01 best_iteration: 00003750 best_val_loss: 5.94e-01 current_epoch: 00000004 current_iteration: 00007500 epochs: 00000007 learning_rate: 6.73e-04 total_steps: 00013125 train_accuracy: 7.27e-01\n",
            "\u001b[2m\u001b[36m(179d_789f_134e pid=3591)\u001b[0m 2023-09-22 22:25:00: val_loss: 6.11e-01 val_accuracy: 0.701700 train_loss: 8.60e-01 best_iteration: 00003750 best_val_loss: 5.94e-01 current_epoch: 00000004 current_iteration: 00007500 epochs: 00000007 learning_rate: 6.73e-04 total_steps: 00013125 train_accuracy: 6.97e-01\n",
            "\u001b[2m\u001b[36m(179d_789f_134e pid=3591)\u001b[0m 2023-09-22 22:25:24: Evaluation Step [5] val_loss: 6.45e-01 val_accuracy: 0.680000 train_loss: 8.70e-01 best_iteration: 00003750 best_val_loss: 5.94e-01 current_epoch: 00000005 current_iteration: 00009375 epochs: 00000007 learning_rate: 3.58e-04 total_steps: 00013125 train_accuracy: 6.97e-01\n",
            "\u001b[2m\u001b[36m(179d_789f_134e pid=3591)\u001b[0m 2023-09-22 22:25:24: val_loss: 6.45e-01 val_accuracy: 0.680000 train_loss: 8.70e-01 best_iteration: 00003750 best_val_loss: 5.94e-01 current_epoch: 00000005 current_iteration: 00009375 epochs: 00000007 learning_rate: 3.58e-04 total_steps: 00013125 train_accuracy: 7.07e-01\n",
            "\u001b[2m\u001b[36m(179d_789f_134e pid=3591)\u001b[0m 2023-09-22 22:25:52: Evaluation Step [6] val_loss: 6.28e-01 val_accuracy: 0.694500 train_loss: 8.76e-01 best_iteration: 00003750 best_val_loss: 5.94e-01 current_epoch: 00000006 current_iteration: 00011250 epochs: 00000007 learning_rate: 9.93e-05 total_steps: 00013125 train_accuracy: 7.07e-01\n",
            "\u001b[2m\u001b[36m(179d_789f_134e pid=3591)\u001b[0m 2023-09-22 22:25:52: val_loss: 6.28e-01 val_accuracy: 0.694500 train_loss: 8.76e-01 best_iteration: 00003750 best_val_loss: 5.94e-01 current_epoch: 00000006 current_iteration: 00011250 epochs: 00000007 learning_rate: 9.93e-05 total_steps: 00013125 train_accuracy: 7.07e-01\n",
            "\u001b[2m\u001b[36m(FileLogger pid=1050)\u001b[0m 2023-09-22 22:26:15:  - Finished training - 179d_789f\n",
            "\u001b[2m\u001b[36m(179d_789f_134e pid=3591)\u001b[0m 2023-09-22 22:26:15: Evaluation Step [7] val_loss: 6.26e-01 val_accuracy: 0.697900 train_loss: 8.67e-01 best_iteration: 00003750 best_val_loss: 5.94e-01 current_epoch: 00000007 current_iteration: 00013125 epochs: 00000007 learning_rate: 4.00e-09 total_steps: 00013125 train_accuracy: 7.07e-01\n",
            "\u001b[2m\u001b[36m(179d_789f_134e pid=3591)\u001b[0m 2023-09-22 22:26:15: val_loss: 6.26e-01 val_accuracy: 0.697900 train_loss: 8.67e-01 best_iteration: 00003750 best_val_loss: 5.94e-01 current_epoch: 00000007 current_iteration: 00013125 epochs: 00000007 learning_rate: 4.00e-09 total_steps: 00013125 train_accuracy: 6.93e-01\n",
            "2023-09-22 22:26:18:  - There are 3 complete trials. with ids: [1, 2, 3]\n"
          ]
        }
      ],
      "source": [
        "shutil.rmtree(parallel_config.experiment_dir, ignore_errors=True)\n",
        "\n",
        "wrapper = MyModelWrapper(\n",
        "    model_class=MyModel,\n",
        ")\n",
        "\n",
        "ablator = ParallelTrainer(\n",
        "    wrapper=wrapper,\n",
        "    run_config=parallel_config,\n",
        ")\n",
        "\n",
        "ablator.launch(working_directory = os.getcwd())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
